services:
  # Ollama service for LLM
  ollama:
    image: ollama/ollama:latest
    container_name: pdf-chat-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    # This will automatically pull models on first run
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      ollama serve &
      sleep 10
      echo 'Downloading models... this may take 10-15 minutes on first run'
      ollama pull llama2 || echo 'llama2 download failed'
      ollama pull nomic-embed-text || echo 'nomic-embed-text download failed'
      echo 'Models ready!'
      wait
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version", "||", "exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 300s  # Give 5 minutes for model downloads
    networks:
      - pdf-chat-network

  # PDF Chat API
  pdf-chat-api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: pdf-chat-api
    ports:
      - "8000:8000"
    volumes:
      - ./uploads:/app/uploads
      - ./vector_stores:/app/vector_stores
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    networks:
      - pdf-chat-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Nginx for serving frontend and reverse proxy
  nginx:
    image: nginx:alpine
    container_name: pdf-chat-nginx
    ports:
      - "80:80"
    volumes:
      - ./frontend:/usr/share/nginx/html
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - pdf-chat-api
    networks:
      - pdf-chat-network
    restart: unless-stopped

volumes:
  ollama_data:
    driver: local

networks:
  pdf-chat-network:
    driver: bridge